{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Predicting Curl Quality\"\nauthor: \"Donald Hescht\"\ndate: \"11/29/2016\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE, message=F, warning=F)\n```\n```{r echo = FALSE}\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(pgmm)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(gbm)\nlibrary(e1071)\nlibrary(corrplot)\n\n```\n\n```{r echo = FALSE}\n# Read and clean data set.  Remove all columns that have NA or divide by zero.\nset.seed(19651)\nliftData <- read.csv(\"pml-training.csv\", na.strings=c(\"#DIV/0\", '', 'NA') ,stringsAsFactors = F)\nliftData <- subset(liftData, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))\nliftData$classe <- as.factor(liftData$classe)\nliftData <- liftData[,colSums(is.na(liftData))==0]\n\n#split ito training and testing \ninTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]\nTraining <- liftData[ inTrain,]\nTesting <- liftData[-inTrain,]\n```\n\n# Summary\nThis paper describes the author's approach to designing a 99% accurate model for predicting curl exercise type as defined in the paper \"Qualitative Activity Recognition of Weight Lifting Exercises\" [1].  This activity's data was captured by sampling 6 lifters with 4 attached sensors.  These sensors were attached to the lifter's forearm, arm, belt and dumbbell.  They were then monitored while performing 5 distinct curl exercises: one that was \"correct\" and four others with distinct errors.  The aggregate of these sensor data and post calculations resulted in 19622 observations of 160 variables.   \n\n# Cleaning and Analysis\nThe lift data contains N/As and divide by zero errors.  These values were caused by post calculations to create summary type data (max/mins and other stats) from the original sensors data.  After removing these feature columns the actual sensor information still present.  Post cleaning two data sets were created: Training (`r dim(Training)`) and Testing (`r dim(Testing)`).   \n\nTo Correlation Plot (with main diagonal of ones removed) shows the \"cleaned\" 53 features correlated to the \"classe\"\" output.  Notice that there is no strong correlation with the \"classe\" out come.  This indicates quite a few input variables will be required to create a 99% accurate model.  This feature selection occurs in the section.\n\n```{r echo = FALSE, fig.height=8, fig.width=8}\n# Look at correlation of classe to Features\nx <-matrix(LETTERS[1:9], ncol=3)\nLD.CorrM <- Training\nLD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))\nLD.Corr <- cor(LD.CorrM[,-1])\ndiag(LD.Corr)<-0\ncorrplot(LD.Corr)\n```\n\n# Model Generation\n\n```{r echo = FALSE}\n# Create feature formula that correlate highest while still providing 99% accuracy \nAllFeatures <- names(sort(abs(LD.Corr[\"classe\",]), decreasing = TRUE))\nFeatures <- AllFeatures[1:37]\nFormula <- as.formula(paste(\"classe\", paste(Features, collapse=\"+\"), sep = \" ~ \"))\n```\n\nThe author used the Random Forest algorithm for its accuracy and general lack of bias.  (The reduction in bias is a result of it keeping out a portion of the training data to ensure good out of sample accuracy.)  The features for the Random Forest were then found by ordering by their absolute correlation value (as found in the previous section). The author then off-line picked the highest correlated features to reduce features from `r length(AllFeatures )` to  `r length(Features)` while keeping 99% accuracy.  \n\nThe final Random Forest **features = ** \n\n**{ `r Features`}**\n\n```{r echo=TRUE}\nset.seed(19652)\nTraining.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)\nTraining.Rf\n```\n\nThe **Random Forest output above predicts** the Out Of Bag error (OOB) or **out of sample error is less than 1%**. The final model individual classification errors (A-E) are plotted below showing each run close to a 99% accuracy. (A later project might consider the effect of reducing the number of trees but this was not performed since time to build the model was not specified.) \n\n```{r echo=FALSE}\nplot(Training.Rf, main=\"Classification Errors For Each classe\")\n```\n\nThe importance of features to accuracy is shown below.  Removing even the least \"important\" features edged the model to approach or cross the required accuracy of 99%.  Likewise, though there is a strong cross correlation between the Euler Angles and the raw values, removing them quickly caused the model accuracy to reduce below 99% accuracy. Therefore, the author kept the correlation ordered `r length(Features)` features.\n\n\n```{r echo=FALSE, fig.height=8, fig.width=6}\nvarImpPlot(Training.Rf, sort=TRUE, type = 1)\n```\n\n# Cross Validation\n\n```{r echo = TRUE}\n# Function to report accuracy of model\nset.seed(19653)\nTraining.Test.Pred <- predict(Training.Rf, Testing)\nctable <- table(Training.Test.Pred, Testing$classe) \nTesting.Acc <-  sum(diag(ctable)) / sum(ctable)\nTraining.OOB.Acc <- sum(diag(Training.Rf$confusion[,1:5])) / sum(Training.Rf$confusion[,1:5])\n```\n\nThe Random Forest model was validated by using a 25% hold out of the \"pml-training.csv\" training data. Using this hold out data gives an out of sample accuracy of **`r Testing.Acc `**. This closely matches the accuracy given by the Random Forest Training Confusion Matrix **(`r Training.OOB.Acc`)** which is ~(1 - OOB error). \n\n# Test Data\nRunning the 20 test cases.\n```{r echo = TRUE}\nset.seed(19654)\ntestData <- read.csv(\"pml-testing.csv\", na.strings=c(\"#DIV/0\", '', 'NA') ,stringsAsFactors = F)\npredict(Training.Rf, testData)\n\n```\n# Citations\n[1] Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz4QYU9sHrZ\n\n[2] https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\n\n",
    "created" : 1479467451391.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "730984767",
    "id" : "471B7170",
    "lastKnownWriteTime" : 1480428551,
    "last_content_update" : 1480428551709,
    "path" : "~/Documents/Coursera/datasciencecoursera/datasciencecoursera/MachineLearning/index.Rmd",
    "project_path" : "index.Rmd",
    "properties" : {
        "ignored_words" : "classe\n",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}