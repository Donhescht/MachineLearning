accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuacy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred1 <- predict(Training.Rf1, Testing)
accuracyMeasures(Training.Test.Pred1, Testing$classe, "Random Forest")
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
# Reduce columns to calculated feature summaries
set.seed(19651)
Training <- Training[,colSums(is.na(Training))==0]
Testing <- Testing[,colSums(is.na(Testing))==0]
Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Look at correlation of variable to conclude Features
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
#set Features to highly correlated values.
#
Training$classe <- as.factor(Training$classe)
Testing$classe <- as.factor(Testing$classe)
Features <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- Features[-(20:52)]
Training <- Training[,Features]
Testing <- Testing[,Features]
Training.Rf1 <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
```
# Model Generation
# Validation
The follow cross validation is not required for Ramdom Forest since the Random Forest algorithm holds out data to perform Out Of Bag (OOB) measurements.  However, to validate the OOB we will compare the Random Forest OOB measures to the validation using the 25% hold out data.
```{r echo = FALSE}
# Function to report accuracy of model
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(truth=truth, pred=pred)
accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuacy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred1 <- predict(Training.Rf1, Testing)
accuracyMeasures(Training.Test.Pred1, Testing$classe, "Random Forest")
```
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
# Reduce columns to calculated feature summaries
set.seed(19651)
Training <- Training[,colSums(is.na(Training))==0]
Testing <- Testing[,colSums(is.na(Testing))==0]
Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Look at correlation of variable to conclude Features
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
#set Features to highly correlated values.
#
Training$classe <- as.factor(Training$classe)
Testing$classe <- as.factor(Testing$classe)
Features <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- Features[-(35:52)]
Training <- Training[,Features]
Testing <- Testing[,Features]
Training.Rf1 <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
```
# Model Generation
# Validation
The follow cross validation is not required for Ramdom Forest since the Random Forest algorithm holds out data to perform Out Of Bag (OOB) measurements.  However, to validate the OOB we will compare the Random Forest OOB measures to the validation using the 25% hold out data.
```{r echo = FALSE}
# Function to report accuracy of model
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(truth=truth, pred=pred)
accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuacy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred1 <- predict(Training.Rf1, Testing)
accuracyMeasures(Training.Test.Pred1, Testing$classe, "Random Forest")
length(AllFeature )
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[-(35:52)]
length(AllFeatures )
length(lFeatures)
length(Features)
Features
Training.Rf1 <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf1$inbag
Training.Rf1$oob.times
Training.Rf1$confusion
Training.Rf1
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[-(30:52)]
Training <- Training[,Features]
Testing <- Testing[,Features]
Training.Rf1 <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
```
```{r echo = FALSE}
# Reduce columns to calculated feature summaries
set.seed(19651)
Training <- Training[,colSums(is.na(Training))==0]
Testing <- Testing[,colSums(is.na(Testing))==0]
Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Training$classe <- as.factor(Training$classe)
Testing$classe <- as.factor(Testing$classe)
```
```{r echo = FALSE}
# Look at correlation of variable to conclude Features
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[-(30:52)]
Training <- Training[,Features]
Testing <- Testing[,Features]
Training.Rf1 <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf1
Training.Rf1$err.rate
Training.Rf1$test
Training.Rf1
Training.Rf1$confusion
Training.Rf1$err.rate
`r Training.Rf`
Training.Rf
Training.Rf <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf
Testing <- Testing[,Features]
Training.Rf <- randomForest(classe ~ ., data=Training, importance=TRUE)
library(ggplot2)
library(caret)
library(randomForest)
library(pgmm)
library(rpart)
library(rpart.plot)
library(gbm)
library(e1071)
library(corrplot)
Training.Rf <- randomForest(classe ~ ., data=Training, importance=TRUE)
Training.Rf
Training.Rf <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
set.seed(19652)
Training.Rf <- randomForest(classe ~ ., data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(truth=truth, pred=pred)
accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuacy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred <- predict(Training.Rf, Testing)
accuracyMeasures(Training.Test.Pred, Testing$classe, "Random Forest")
Features <- paste(AllFeatures[-(35:52)], collapse="+")
Features
Training.Rf <- randomForest(classe ~ Features, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Formula <- as.formula(paste("classe", Features, sep = " ~ "))
Formula
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
set.seed(19651)
Training <- Training[,colSums(is.na(Training))==0]
Testing <- Testing[,colSums(is.na(Testing))==0]
Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Training$classe <- as.factor(Training$classe)
Testing$classe <- as.factor(Testing$classe)
```
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
#QuizTesting <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
inTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
set.seed(19651)
Training <- Training[,colSums(is.na(Training))==0]
Testing <- Testing[,colSums(is.na(Testing))==0]
Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
Training$classe <- as.factor(Training$classe)
Testing$classe <- as.factor(Testing$classe)
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- paste(AllFeatures[-(35:52)], collapse="+")
Formula <- as.formula(paste("classe", Features, sep = " ~ "))
set.seed(19652)
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf
AllFeatures
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[-(35:52)]
Formula <- as.formula(paste("classe", paste(Features, collapse="+"), sep = " ~ "))
Formula
Features
AllFeatures
Features <- AllFeatures[1:35]
Features
Formula <- as.formula(paste("classe", paste(Features, collapse="+"), sep = " ~ "))
Formula
set.seed(19652)
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE, do.trace=100)
Training.Rf
plot(Training.Rf)
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(truth=truth, pred=pred)
accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuacy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred <- predict(Training.Rf, Testing)
accuracyMeasures(Training.Test.Pred, Testing$classe, "Random Forest")
Training.Test.Pred <- predict(Training.Rf, Testing)
Testing.Acc = accuracyMeasures(Training.Test.Pred, Testing$classe, "Random Forest")
Testing.Acc
accuracyMeasures <- function(pred, truth, name="model"){
ctable <- table(truth=truth, pred=pred)
accuracy <- sum(diag(ctable)) / sum(ctable)
precision <- ctable[2,2] / sum(ctable[,2])
recall <- ctable[2,2] / sum(ctable[2,])
f1 <- precision*recall
data.frame(Model=name, Accuracy=accuracy, f1=f1, TestSize = length(pred) )
}
Training.Test.Pred <- predict(Training.Rf, Testing)
Testing.Acc = accuracyMeasures(Training.Test.Pred, Testing$classe, "Random Forest")
Testing.Acc
Testing.Acc$Accuracy
varImpPlot(Training.Rf,
sort = T,
main="Variable Importance",
n.var=length(Features))
varImpPlot(Training.Rf,
sort = T,
main="Variable Importance",
n.var=length(Features),
type = 1)
length(Features)
x = 1:35
AllFeatures[x]
y = 1; z = 35
x = y:z
x
AllFeatures[x]
Features <- AllFeatures[1:36]
Training.Rf$err.rate
sum(Training.Rf$err.rate)
sum(Training.Rf$err.rate)/length(Training.Rf$err.rate)
sum(Training.Rf$err.rate[500,])
Training.Rf$confusion
Training.Rf$confusion[,6]
sum(Training.Rf$confusion[,6])
sum(Training.Rf$confusion[,6])/5
QuizTesting <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
Formula
Formula
typeof(Formula)
as.char(Formula)
as.character(Formula)
Formula[1]
Formula[2]
Formula[3]
set.seed(19652)
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
liftData <- subset(liftData, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
liftData$classe <- as.factor(liftData$classe)
liftData <- lift[,colSums(is.na(liftData))==0]
# Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Training$classe <- as.factor(Training$classe)
# Testing$classe <- as.factor(Testing$classe)
#split ito training and testing
inTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
library(ggplot2)
library(caret)
library(randomForest)
library(pgmm)
library(rpart)
library(rpart.plot)
library(gbm)
library(e1071)
library(corrplot)
set.seed(19651)
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
liftData <- subset(liftData, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
liftData$classe <- as.factor(liftData$classe)
liftData <- lift[,colSums(is.na(liftData))==0]
# Training <- subset(Training, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Testing <- subset(Testing, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
# Training$classe <- as.factor(Training$classe)
# Testing$classe <- as.factor(Testing$classe)
#split ito training and testing
inTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
# Training <- Training[,colSums(is.na(Training))==0]
# Testing <- Testing[,colSums(is.na(Testing))==0]
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
liftData <- subset(liftData, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
liftData$classe <- as.factor(liftData$classe)
liftData <- liftData[,colSums(is.na(liftData))==0]
inTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[1:37]
Formula <- as.formula(paste("classe", paste(Features, collapse="+"), sep = " ~ "))
set.seed(19652)
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
ctable <- table(truth=truth, pred=pred)
ctable = table(Training.Test.Pred, Testing$classe)
ctable
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Testing.Acc
# Function to report accuracy of model
Training.Test.Pred <- predict(Training.Rf, Testing)
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Training.Test.Pred <- predict(Training.Rf, Testing)
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Testing.Acc
set.seed(19653)
Training.Test.Pred <- predict(Training.Rf, Testing)
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Testing.Acc
set.seed(19653)
Training.Test.Pred <- predict(Training.Rf, Testing)
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Testing.Acc
set.seed(19653)
Training.Test.Pred <- predict(Training.Rf, Testing)
ctable <- table(Training.Test.Pred, Testing$classe)
Testing.Acc = sum(diag(ctable)) / sum(ctable)
Testing.Acc
postResample(Training.Rf, Testing)
postResample(Training.Rf, Testing)
postResample(Training.Rf, Testing$classe)
postResample(Training.Test.Pred, Testing$classe)
x = postResample(Training.Test.Pred, Testing$classe)
x
set.seed(19653)
Training.Test.Pred <- predict(Training.Rf, Testing)
Testing.Acc = postResample(Training.Test.Pred, Testing$classe)
Testing.Acc[1]
Training.Rf$confusion
Training.OOB.Acc <- 1 - (sum(diag(Training.Rf$confusion)) / sum(Training.Rf$confusion))
Training.OOB.Acc
sum(diag(Training.Rf$confusion)) / sum(Training.Rf$confusion)
Testing.Acc$Accuracy
Testing.Acc
Training.OOB.Acc
Testing.Acc <-  sum(diag(ctable)) / sum(ctable)
Testing.Acc
Testing.Acc <-  sum(diag(ctable)) / sum(ctable)
Testing.Acc
Training.OOB.Acc
Training.OOB.Acc
sum(diag(Training.Rf$confusion))
sum(Training.Rf$confusion)
Training.Rf$confusion
Training.Rf$confusion[1:5]
Training.Rf$confusion[1:5,1]
Training.Rf$confusion[,1:5]
Training.Rf$confusion[,1:5]
Training.OOB.Acc <- sum(diag(Training.Rf$confusion[,1:5])) / sum(Training.Rf$confusion[,1:5])
Training.OOB.Acc
Testing.Acc
dim(Testing)
dim(Training)
dim(Testing)
plot(Training.Rf)
varImpPlot(Training.Rf, sort=TRUE, type = 1, main="Mean Decreasing Accuracy")
varImpPlot(Training.Rf, sort=TRUE, type = 1)
dev.off
dev.off()
dev.off('pdf')
dev = 'pdf'
dev.off()
dev.on()
png()
dev.list()
dev.off()
dev.list()
png(quality=150)
png(pointsize=8)
?plot
varImpPlot(Training.Rf, sort=TRUE, type = 1)
varImpPlot(Training.Rf, sort=TRUE, type = 1)
dev.off()
plot(Training.Rf)
varImpPlot(Training.Rf, sort=TRUE, type = 1)
Features
F <- Features[-20]
Formula <- as.formula(paste("classe", paste(F, collapse="+"), sep = " ~ "))
set.seed(19652)
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
F
F <- F[-1]
Formula <- as.formula(paste("classe", paste(F, collapse="+"), sep = " ~ "))
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
F
F <- F[-18]
Formula <- as.formula(paste("classe", paste(F, collapse="+"), sep = " ~ "))
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
testtesting <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
test.pred <- predict(Training.Rf, testtesting)
testData <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
predict(Training.Rf, testData)
testData <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
library(ggplot2)
library(caret)
library(randomForest)
library(pgmm)
library(rpart)
library(rpart.plot)
library(gbm)
library(e1071)
library(corrplot)
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
library(ggplot2)
library(caret)
library(randomForest)
library(pgmm)
library(rpart)
library(rpart.plot)
library(gbm)
library(e1071)
library(corrplot)
testData <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
T
F
F <- FALSE
testData <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
set.seed(19651)
liftData <- read.csv("pml-training.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
liftData <- subset(liftData, select = -c(X, raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp, new_window, num_window))
liftData$classe <- as.factor(liftData$classe)
liftData <- liftData[,colSums(is.na(liftData))==0]
#split ito training and testing
inTrain <- createDataPartition(liftData$classe, p = 3/4)[[1]]
Training <- liftData[ inTrain,]
Testing <- liftData[-inTrain,]
x <-matrix(LETTERS[1:9], ncol=3)
LD.CorrM <- Training
LD.CorrM$classe <- as.numeric(match(LD.CorrM$classe,x))
LD.Corr <- cor(LD.CorrM[,-1])
diag(LD.Corr)<-0
corrplot(LD.Corr)
AllFeatures <- names(sort(abs(LD.Corr["classe",]), decreasing = TRUE))
Features <- AllFeatures[1:37]
Formula <- as.formula(paste("classe", paste(Features, collapse="+"), sep = " ~ "))
set.seed(19652)
Training.Rf <- randomForest(Formula, data=Training, mtry=as.integer(sqrt(length(Features))), importance=TRUE)
Training.Rf
set.seed(19654)
testData <- read.csv("pml-testing.csv", na.strings=c("#DIV/0", '', 'NA') ,stringsAsFactors = F)
predict(Training.Rf, testData)
install.packages("wordCountRMD")
install.packages("wordCountaddin")
install.packages("wordcountaddin")
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
install.packages("devtools")
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
install.packages("purrr")
